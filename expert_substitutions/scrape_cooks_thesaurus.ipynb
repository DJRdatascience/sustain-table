{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "chkpt_fname = 'cooksthesaurus_pickle.checkpoint'\n",
    "df_fname = 'cooksthesaurus_pickle_df.pkl'\n",
    "unpack_subcat = ['onions','fruit vegetables', 'cheese','herbs','spices','herb & spice mixes','condiments','liquers','wines','liquors','pasta',\n",
    "                'asian noodles', 'other noodles','beef','pork','lamb','veal','cured meats','variety meats','lean, flaky-textured fish',\n",
    "                ] #These subcategories have another layer of subcategories that we need to go through\n",
    "keep_first_scrape = ['wine','pasta','asian noodles', 'other noodles'] #These subcategories have useful information on the landing page, so we keep the information\n",
    "skip_subcat = ['africa','america','asia','europe','hispanic countries','india','middle east', 'world'] #These subcategories are unpacked separately (above)\n",
    "skip_cat = ['equipment'] #These categories have no useful information and are therefore skipped\n",
    "\n",
    "def _load( fname=chkpt_fname ):\n",
    "    with open( fname, 'rb' ) as f:\n",
    "        return pickle.load( f ).__dict__\n",
    "def _dump( obj: object, fname=chkpt_fname ):\n",
    "    with open( fname, 'wb' ) as f:\n",
    "        pickle.dump( obj,  f )\n",
    "\n",
    "class CooksThesaurusScraper:\n",
    "\n",
    "    def __init__( self, initialize=True ):\n",
    "        self.initialize = initialize\n",
    "        if self.initialize: # If starting a new scrape\n",
    "            self.base_url = 'http://www.foodsubs.com'\n",
    "            self.headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"}\n",
    "            self.cols = [ 'cat', 'cat_url', 'subcat', 'subcat_url', 'subcat_img', 'item', 'item_equivs', 'item_subs', 'item_img' ] # Output dataframe columns\n",
    "            self.df = self._create_df( [] ) # Stores results\n",
    "            self.tags = { 'cat':[], 'subcat':[] } # Category and subcategory tags\n",
    "            self.i = { 'cat':0, 'subcat':0 } # Category and subcategory index, for restarting scrape (current)\n",
    "            self.restart = { 'cat':0, 'subcat':0 } # Category and subcategory index, for restarting scrape (restart here)\n",
    "            self.cat = { 'cat':'', 'subcat':'' } # Category and subcategory name\n",
    "            self.url = { 'cat':'', 'subcat':'' } # Category and subcategory urls\n",
    "            self.select_by = { 'cat':'td ul li a', 'subcat':'font a', 'subcat_old':'font p font a' } # Used for scraping categories and subcategories\n",
    "            self.scraped_subcats = [] # Records urls of scraped subcats so we don't scrape them multiple times (sometimes a subcat belongs to more than one category)\n",
    "        else: # If restarting a scrape\n",
    "            self.__dict__ = _load()\n",
    "            if not self.df.empty: # Make sure that we recorded some results\n",
    "                self.initialize = False\n",
    "\n",
    "    def _create_df( self, data: list ) -> object:\n",
    "        return pd.DataFrame( data=data, columns=self.cols )\n",
    "\n",
    "    def _create_soup( self, url: str ) -> object: \n",
    "        request = requests.get( url=url, headers=self.headers )\n",
    "        time.sleep( 2 )\n",
    "        return BeautifulSoup( request.content, features='html.parser' )\n",
    "\n",
    "    def _get_categories( self, cat_choice:str ): #url: str, select_by: str\n",
    "        if cat_choice == 'cat': #Use base url to get categories\n",
    "            url = self.base_url\n",
    "        else: #Use category url to get subcategories\n",
    "            url = self.url['cat']\n",
    "        soup = self._create_soup( url )\n",
    "        tags = soup.select( self.select_by[cat_choice] )\n",
    "        self.tags[cat_choice] = [t for t in tags if 'www' not in t['href'] and t.text.lower().replace('\\n',' ').strip() not in skip_subcat]\n",
    "\n",
    "    def _get_list( self, txt: str, split_on: str ) -> list:\n",
    "        txt = re.sub( r'\\s+', ' ', txt ).lower()\n",
    "        txt = re.sub( r'\\([^)]*\\)', '', txt ).strip()\n",
    "        txt_list = re.split( split_on,  txt )\n",
    "        return [ t for t in txt_list if t ]\n",
    "\n",
    "    def _get_item( self, soup_list: list ) -> list: #This is where most of the parsing logic is\n",
    "        main = []\n",
    "        equiv = []\n",
    "        sub = []\n",
    "        link = ''\n",
    "        for i, s in enumerate( soup_list ):\n",
    "            if main == [] and 'b' in [s.parent.name, s.parent.parent.name] and s[:5].lower().strip() not in ['subst','notes']:\n",
    "                main = self._get_list( s.text, r'\\s*=\\s*' )\n",
    "                if 'href' in s.parent.attrs:\n",
    "                    link = ''.join([self.base_url,'/',s.parent['href']])\n",
    "                elif 'href' in s.parent.parent.attrs:\n",
    "                    link = ''.join([self.base_url,'/',s.parent.parent['href']])\n",
    "            elif main != [] and '=' in s.text[:10] and not re.search(r'\\d', s.text[:10]): #Sometimes the synonyms are broken up, so this is an effort to recombine #re.search('^\\s*?=',s):\n",
    "                if re.search('^\\s*?=',s.text): # if the string starts with \"=\", we know a name was not broken up\n",
    "                    main.extend( self._get_list( s.text, r'\\s*=\\s*' ) )\n",
    "                else: # if the string does not start with \"=\", a name was likely broken up\n",
    "                    main = self._get_list( s.text+soup_list[i-1].text, r'\\s*=\\s*' )\n",
    "            elif 'equiv' in s[:12].lower():\n",
    "                if len(soup_list) - (i+1):\n",
    "                    equiv = self._get_list( soup_list[i+1].text, r'\\s*=\\s*' )\n",
    "            elif 'subst' in s[:12].lower():\n",
    "                if len(soup_list) - (i+1):\n",
    "                    sub = self._get_list( soup_list[i+1].text, r'\\s+or\\s+' )\n",
    "        return [main,equiv,sub, self.item_img], link\n",
    "    \n",
    "    def _tag_iter(self):\n",
    "        soup = self._create_soup( self.url['subcat'] )\n",
    "        tags = soup.select( 'blockquote table tr td' )\n",
    "        self.base_item = [ self.cat['cat'], self.url['cat'], self.cat['subcat'], self.url['subcat'], self.img ]\n",
    "        self.item_img = ''\n",
    "        item = [[],[],[], '']\n",
    "        self.items, self.links = [], []\n",
    "        for tag in tags:\n",
    "            text_finder = tag.find_all(text=True, recursive=True)\n",
    "            text_finder = [ t for t in text_finder if re.sub('\\s+','',t)]\n",
    "            if text_finder:\n",
    "                item, link = self._get_item( text_finder )\n",
    "                if item[0]:\n",
    "                    self.item_img = ''\n",
    "                    self.items.append( self.base_item+item )\n",
    "                    self.links.append(link)\n",
    "            else:\n",
    "                image_finder = tag.find('img')\n",
    "                if image_finder:\n",
    "                    self.item_img = self.base_url+image_finder['src']\n",
    "                    if not self.base_item[-1]:\n",
    "                        self.base_item[-1] = self.item_img\n",
    "\n",
    "    def _unpack_sub(self) -> object:\n",
    "        if self.cat['subcat'] in keep_first_scrape:\n",
    "            self.df = pd.concat( [self.df,self._create_df( self.items )] )\n",
    "        subcat_prefix = self.cat['subcat']\n",
    "        for item,link in zip(self.items,self.links):\n",
    "            if link != '':\n",
    "                self.cat['subcat'] = ''.join([subcat_prefix,', ',item[5][0]])\n",
    "                self.url['subcat'] = link\n",
    "                self._scrape_sub(img=item[-1])\n",
    "\n",
    "    def _scrape_sub( self, img='' ) -> object:\n",
    "        self.img = img\n",
    "        self.scraped_subcats.append( self.url['subcat'] )\n",
    "        self._tag_iter()\n",
    "        if self.cat['subcat'] in unpack_subcat:\n",
    "            self._unpack_sub()\n",
    "        else:\n",
    "            self.df = pd.concat( [self.df,self._create_df( self.items )] )\n",
    "\n",
    "    def _make_chkpt( self ):\n",
    "        _dump( self )\n",
    "        self.df.to_pickle( df_fname )\n",
    "\n",
    "    def _set_restart( self, cat_choice ):\n",
    "        if cat_choice == 'subcat':\n",
    "            if len(self.tags['subcat'])-(self.i['subcat']+1):\n",
    "                cat_i, subcat_i = 0, 1\n",
    "            elif len(self.tags['cat'])-(self.i['cat']+1):\n",
    "                cat_i, subcat_i = 1, -self.i['subcat']\n",
    "            else:\n",
    "                cat_i, subcat_i = 0, 1\n",
    "        else:\n",
    "            cat_i, subcat_i = 0, -self.i['subcat']\n",
    "        self.restart['cat'] = self.i['cat']+cat_i\n",
    "        self.restart['subcat'] = self.i['subcat']+subcat_i\n",
    "    \n",
    "    def _iter( self, cat_choice: str, single_cat='', debug=False ):\n",
    "        if self.initialize: #If we have initialized the scrape, we always start from beginning \n",
    "            start = 0\n",
    "        else: #If we restart a scrape, we start from where we left off\n",
    "            start = self.restart[cat_choice]\n",
    "        for i, tag in enumerate( self.tags[cat_choice][start:] ):\n",
    "            self.i[cat_choice] = i\n",
    "            self.cat[cat_choice] = tag.text.lower().replace('\\n',' ').strip()\n",
    "            self.url[cat_choice] = ''.join( [self.base_url,'/',tag['href']] )\n",
    "            if cat_choice == 'subcat' and not debug and not (self.url['subcat'] in self.scraped_subcats):\n",
    "                print( f'Scraping: {self.url[cat_choice]}' )\n",
    "                self._scrape_sub()\n",
    "                if self.save:\n",
    "                    self._make_chkpt()\n",
    "            elif self.url[cat_choice]==single_cat or (cat_choice=='cat' and not single_cat): # Only triggers once if scraping single category\n",
    "                self._set_restart('cat')\n",
    "                if self.cat[cat_choice] not in skip_cat: # We skip some categories that do not contain useful information\n",
    "                    self._get_categories('subcat')\n",
    "                    self._iter( 'subcat', debug=debug )\n",
    "                if single_cat:\n",
    "                    break\n",
    "            if cat_choice == 'subcat' and self.save:\n",
    "                self._set_restart('subcat')\n",
    "\n",
    "    def scrape( self ): #This is used to scrape the entire website\n",
    "        self.save = True #Results are saved, and the state of the scraper is saved (for restarting a crashed scraper)\n",
    "        '''\n",
    "        Running this returns nothing. The results of the scrape are stored in .df as a dataframe, and they are saved as 'cooks_thesaurus_results.csv' in\n",
    "        the current working directory.\n",
    "        '''\n",
    "        if self.initialize:\n",
    "            self._get_categories('cat')\n",
    "        else:\n",
    "            self._iter('subcat')\n",
    "        self._iter('cat')\n",
    "\n",
    "    def scrape_subcat( self, page_url: str, subcat='' ): #This is used to scrape a specific subcategory (used mostly for debugging)\n",
    "        self.save = False #Results are not saved, and the state of the scraper is not saved\n",
    "        '''\n",
    "        page_url (str): Required. This is the url of the subcategory page.\n",
    "        subcat (str): Optional. Name of the subcategory being scraped (all lowercase). If this is given, the scraper will check to see if subcategory\n",
    "                                contains nested subcategories, and if so, it will unpack them.\n",
    "        \n",
    "        Running this returns nothing. The results of the scrape are stored in .df as a dataframe.\n",
    "        '''\n",
    "        self.url['subcat'] = page_url\n",
    "        self.cat['subcat'] = subcat\n",
    "        self._scrape_sub()\n",
    "    \n",
    "    def scrape_cat( self, page_url: str, debug=False ): #This is used to scrape a specific category (used mostly for debugging)\n",
    "        self.save = False #Results are not saved, and the state of the scraper is not saved\n",
    "        '''\n",
    "        page_url (str): Required. This is the url of the category page.\n",
    "        debug (bool): Optional. If True, the pages will not actually be scraped (they will simply be iterated over). This is for debugging purposes.\n",
    "        \n",
    "        Running this returns nothing. The results of the scrape are stored in .df as a dataframe.\n",
    "        '''\n",
    "        self._get_categories('cat')\n",
    "        self._iter( 'cat', page_url, debug )\n",
    "    \n",
    "    def get_images( self, img_folder_path: str, df_path='' ):\n",
    "        '''\n",
    "        img_folder_path (str): Required. This is the path to folder where the images will be saved.\n",
    "        df_path (str): Optional. If provided, the images from a results dataframe will be scraped. If not provided, results stored in the current\n",
    "                                 object will be used.\n",
    "        \n",
    "        Running this returns nothing. The images will be stored in the desired folder.\n",
    "        '''\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.foodsubs.com/Roots.html\n",
      "Scraping: http://www.foodsubs.com/Tubers.html\n",
      "Scraping: http://www.foodsubs.com/Potatoes.html\n",
      "Scraping: http://www.foodsubs.com/Sweetpotatoes.html\n",
      "Scraping: http://www.foodsubs.com/Stalk.html\n",
      "Scraping: http://www.foodsubs.com/Onions.html\n",
      "Scraping: http://www.foodsubs.com/Garlic.html\n",
      "Scraping: http://www.foodsubs.com/Ginger.html\n",
      "Scraping: http://www.foodsubs.com/Cabbage.html\n",
      "Scraping: http://www.foodsubs.com/Greensld.html\n",
      "Scraping: http://www.foodsubs.com/Greenckg.html\n",
      "Scraping: http://www.foodsubs.com/Vegiesinflor.html\n",
      "Scraping: http://www.foodsubs.com/Snapbean.html\n",
      "Scraping: http://www.foodsubs.com/Pods.html\n",
      "Scraping: http://www.foodsubs.com/Shellpeas.html\n",
      "Scraping: http://www.foodsubs.com/Shellbeans.html\n",
      "Scraping: http://www.foodsubs.com/Mushroom.html\n",
      "Scraping: http://www.foodsubs.com/Fruitvegies.html\n",
      "Scraping: http://www.foodsubs.com/Seaveg.html\n",
      "Scraping: http://www.foodsubs.com/Sprouts.html\n",
      "Scraping: http://www.foodsubs.com/Vegies.html\n",
      "Scraping: http://www.foodsubs.com/Fruitcit.html\n",
      "Scraping: http://www.foodsubs.com/Fruitber.html\n",
      "Scraping: http://www.foodsubs.com/Fruitsto.html\n",
      "Scraping: http://www.foodsubs.com/Fruittro.html\n",
      "Scraping: http://www.foodsubs.com/Fruittroex.html\n",
      "Scraping: http://www.foodsubs.com/Fruitmel.html\n",
      "Scraping: http://www.foodsubs.com/Fruitdry.html\n",
      "Scraping: http://www.foodsubs.com/Fruitoth.html\n",
      "Scraping: http://www.foodsubs.com/Apples.html\n",
      "Scraping: http://www.foodsubs.com/Pears.html\n",
      "Scraping: http://www.foodsubs.com/Fruitpre.html\n",
      "Scraping: http://www.foodsubs.com/Candied.html\n",
      "Scraping: http://www.foodsubs.com/Juice.html\n",
      "Scraping: http://www.foodsubs.com/Dairyoth.html\n",
      "Scraping: http://www.foodsubs.com/Cultmilk.html\n",
      "Scraping: http://www.foodsubs.com/Nondairy.html\n",
      "Scraping: http://www.foodsubs.com/Cheese.html\n",
      "Scraping: http://www.foodsubs.com/Eggs.html\n",
      "Scraping: http://www.foodsubs.com/Herbs.html\n",
      "Scraping: http://www.foodsubs.com/Spice.html\n",
      "Scraping: http://www.foodsubs.com/SpicePepper.html\n",
      "Scraping: http://www.foodsubs.com/Spicemix.html\n",
      "Scraping: http://www.foodsubs.com/Seeds.html\n",
      "Scraping: http://www.foodsubs.com/Extracts.html\n",
      "Scraping: http://www.foodsubs.com/Salt.html\n",
      "Scraping: http://www.foodsubs.com/Sweeten.html\n",
      "Scraping: http://www.foodsubs.com/Syrups.html\n",
      "Scraping: http://www.foodsubs.com/Chocvan.html\n",
      "Scraping: http://www.foodsubs.com/Candy.html\n",
      "Scraping: http://www.foodsubs.com/Nutseed.html\n",
      "Scraping: http://www.foodsubs.com/Condimnt.html\n",
      "Scraping: http://www.foodsubs.com/Vinegars.html\n",
      "Scraping: http://www.foodsubs.com/Liqueurs.html\n",
      "Scraping: http://www.foodsubs.com/Aperitif.html\n",
      "Scraping: http://www.foodsubs.com/Brandy.html\n",
      "Scraping: http://www.foodsubs.com/Wines.html\n",
      "Scraping: http://www.foodsubs.com/Alcohol.html\n",
      "Scraping: http://www.foodsubs.com/Stock.html\n",
      "Scraping: http://www.foodsubs.com/Waters.html\n",
      "Scraping: http://www.foodsubs.com/Rice.html\n",
      "Scraping: http://www.foodsubs.com/GrainWheat.html\n",
      "Scraping: http://www.foodsubs.com/GrainCorn.html\n",
      "Scraping: http://www.foodsubs.com/GrainOats.html\n",
      "Scraping: http://www.foodsubs.com/GrainBarley.html\n",
      "Scraping: http://www.foodsubs.com/GrainBuckwheat.html\n",
      "Scraping: http://www.foodsubs.com/GrainRye.html\n",
      "Scraping: http://www.foodsubs.com/GrainKamut.html\n",
      "Scraping: http://www.foodsubs.com/GrainTrit.html\n",
      "Scraping: http://www.foodsubs.com/GrainSpelt.html\n",
      "Scraping: http://www.foodsubs.com/Grainoth.html\n",
      "Scraping: http://www.foodsubs.com/Flour.html\n",
      "Scraping: http://www.foodsubs.com/Flournw.html\n",
      "Scraping: http://www.foodsubs.com/Nutmeals.html\n",
      "Scraping: http://www.foodsubs.com/Pasta.html\n",
      "Scraping: http://www.foodsubs.com/Noodles.html\n",
      "Scraping: http://www.foodsubs.com/NoodlesOther.html\n",
      "Scraping: http://www.foodsubs.com/Dough.html\n",
      "Scraping: http://www.foodsubs.com/Thicken.html\n",
      "Scraping: http://www.foodsubs.com/ThickenGelatins.html\n",
      "Scraping: http://www.foodsubs.com/ThickenStarch.html\n",
      "Scraping: http://www.foodsubs.com/Wrappers.html\n",
      "Scraping: http://www.foodsubs.com/Bread.html\n",
      "Scraping: http://www.foodsubs.com/Cookies.html\n",
      "Scraping: http://www.foodsubs.com/Cakes.html\n",
      "Scraping: http://www.foodsubs.com/Crackers.html\n",
      "Scraping: http://www.foodsubs.com/Crumbs.html\n",
      "Scraping: http://www.foodsubs.com/Flatbread.html\n",
      "Scraping: http://www.foodsubs.com/Peas.html\n",
      "Scraping: http://www.foodsubs.com/Lentils.html\n",
      "Scraping: http://www.foodsubs.com/Beans.html\n",
      "Scraping: http://www.foodsubs.com/Nuts.html\n",
      "Scraping: http://www.foodsubs.com/Soyprod.html\n",
      "Scraping: http://www.foodsubs.com/BeanProducts.html\n",
      "Scraping: http://www.foodsubs.com/Poultry.html\n",
      "Scraping: http://www.foodsubs.com/Meats.html\n",
      "Scraping: http://www.foodsubs.com/MeatPork.html\n",
      "Scraping: http://www.foodsubs.com/MeatLamb.html\n",
      "Scraping: http://www.foodsubs.com/MeatVeal.html\n",
      "Scraping: http://www.foodsubs.com/MeatOther.html\n",
      "Scraping: http://www.foodsubs.com/Game.html\n",
      "Scraping: http://www.foodsubs.com/Meatcure.html\n",
      "Scraping: http://www.foodsubs.com/Meatvar.html\n",
      "Scraping: http://www.foodsubs.com/MeatDried.html\n",
      "Scraping: http://www.foodsubs.com/Ffirmlea.html\n",
      "Scraping: http://www.foodsubs.com/Ffirmfat.html\n",
      "Scraping: http://www.foodsubs.com/Fflaklea.html\n",
      "Scraping: http://www.foodsubs.com/Fflakfat.html\n",
      "Scraping: http://www.foodsubs.com/Fishsmok.html\n",
      "Scraping: http://www.foodsubs.com/Shelfish.html\n",
      "Scraping: http://www.foodsubs.com/Shelfishcrab.html\n",
      "Scraping: http://www.foodsubs.com/Caviar.html\n",
      "Scraping: http://www.foodsubs.com/Leaven.html\n",
      "Scraping: http://www.foodsubs.com/LeavenYeast.html\n",
      "Scraping: http://www.foodsubs.com/Fatsoils.html\n",
      "Scraping: http://www.foodsubs.com/Oils.html\n",
      "Scraping: http://www.foodsubs.com/Flowers.html\n",
      "Scraping: http://www.foodsubs.com/Misc.html\n",
      "Scraping: http://www.foodsubs.com/Pickles.html\n",
      "Scraping: http://www.foodsubs.com/EqMeasure.html\n",
      "Scraping: http://www.foodsubs.com/EqClean.html\n",
      "Scraping: http://www.foodsubs.com/EqCut.html\n",
      "Scraping: http://www.foodsubs.com/EqBake.html\n",
      "Scraping: http://www.foodsubs.com/EqStove.html\n",
      "Scraping: http://www.foodsubs.com/EqRoast.html\n",
      "Scraping: http://www.foodsubs.com/EqExtract.html\n",
      "Scraping: http://www.foodsubs.com/EqMash.html\n",
      "Scraping: http://www.foodsubs.com/EqOutdoor.html\n",
      "Scraping: http://www.foodsubs.com/EqBar.html\n",
      "Scraping: http://www.foodsubs.com/EqBowls.html\n",
      "Scraping: http://www.foodsubs.com/Equipmt.html\n"
     ]
    }
   ],
   "source": [
    "scraper = CooksThesaurusScraper()\n",
    "scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = CooksThesaurusScraper(False)\n",
    "scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 9)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper = CooksThesaurusScraper()\n",
    "scraper.scrape_subcat( 'http://www.foodsubs.com/Fruitvegies.html','fruit vegetables' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = CooksThesaurusScraper()\n",
    "scraper.scrape_cat( 'http://www.foodsubs.com/FGVegetables.html',debug=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.tags['subcat'][20].text.lower().replace('\\n',' ').strip() in unpack_subcat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30788dc49a0c450f8bd710beb08d90334d11c93adc4c59cb8ced2464dfa7ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
